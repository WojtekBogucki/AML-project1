---
title: "Raport 1"
author: "Wojciech Bogucki, Michał Pastuszka"
date: "28 03 2021"
geometry: margin=1cm
output: 
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.pos = "!H")
library(kableExtra)
library(ggpubr)
library(ggplot2)
library(MASS)
library(class)
library(microbenchmark)
library(dplyr)
source("src/measure.R")
source("src/gradient_descent.R")
source("src/iwls.R")
source("src/util.R")
source("src/optim_logreg.R")
load("data/jain_train.Rda")
load("data/jain_test.Rda")
load("data/spambase_train.Rda")
load("data/spambase_test.Rda")
load("data/mammography_train.Rda")
load("data/mammography_test.Rda")
load("data/skin_seg_train.Rda")
load("data/skin_seg_test.Rda")
load("data/occupancy_train.Rda")
load("data/occupancy_test.Rda")
set.seed(123)


```
# Datasets
```{r data_stats, fig.width=8}
datasets_names <- c("Jain", "Spambase", "Mammography","Skin segmentation", "Occupancy")
nrows<- c(dim(jain_train)[1], dim(spambase_train)[1], dim(mammography_train)[1], dim(skin_seg_train)[1], dim(occupancy_train)[1])
ncols <- c(dim(jain_train)[2], dim(spambase_train)[2], dim(mammography_train)[2], dim(skin_seg_train)[2], dim(occupancy_train)[2]) - 1
class_ratio <- c(mean(jain_train$V3), mean(spambase_train$TARGET), mean(mammography_train$class), mean(skin_seg_train$Class), mean(occupancy_train$Occupancy))
df <- data.frame(datasets_names, nrows, ncols, class_ratio)

kable(df, row.names = FALSE, digits = 2, caption = "Training dataset's dimensions", col.names = c("Dataset name","Number of obeservations", "Number of predictors", "Target class ratio" )) %>% kable_styling(full_width = FALSE, latex_options = "hold_position")

```
All columns were standardized. Datasets were split on training set and test set according to ratio $80:20$.

# Methods

Three main optimization algorithms for logistic regression were tested: **Gradient Descent** (GD), **Stochastic Gradient Descent** (SGD), **Iterated Weighted Least Squares** (IWLS). Two additional algorithms from function `optim()` also were tested: **Conjugate Gradient** (CG), **Broyden–Fletcher–Goldfarb–Shanno** (BFGS)

# Experiments
## Convergence analysis
The aim of this analysis was to compare convergence of different algorithms.

### Learning rate
Firstly, impact of learning rate in gradient descent algorithms was tested on dataset spambase
```{r lr_SGD}
X <- as.matrix(spambase_train[,-ncol(spambase_train)])
Y <- as.matrix(spambase_train[,ncol(spambase_train)])


models <- list()
max_iter <- 100
epsilon <- 1e-4
lr <- c(0.05, 0.01, 0.005, 0.001)
for (i in 1:4){
  models[[i]] <- sgd(X, Y, epsilon , max_iter, lr[i])
}


g1 <- compare_models(models, labels = paste("lr =", lr), title = "Stochastic Gradient Descent")
g1 <- g1 + labs(color="Params") + theme(legend.position="bottom")
```

```{r lr_GD, fig.height=4, fig.width=9, fig.cap="Values of loss during training with different learning rates"}
models <- list()
max_iter <- 1000
epsilon <- 1e-4
lr <- c(1, 0.75, 0.5, 0.3, 0.2, 0.1, 0.05, 0.01)
n <- length(lr)
for (i in 1:n){
  models[[i]] <- gradient_descent(X, Y, epsilon, max_iter, lr[i])

}
g2 <- compare_models(models, labels = paste("lr =", lr), title = "Gradient Descent")
g2 <- g2 + labs(color="Params")  + theme(legend.position="bottom")
ggarrange(g1 , g2, nrow=1)
```

On Figure 1 we can see that SGD is instable for big learning rate and the fastest and the best learning rate equals $0.01$. On the other hand for GD the higher value of learning rate the faster is the algorithm

### Value of log-likelihood function

```{r prep_models_jain, warning=FALSE}
X <- as.matrix(jain_train[,-ncol(jain_train)])
Y <- as.matrix(jain_train[,ncol(jain_train)])

max_iter <- 100
epsilon <- 1e-4


model <- gradient_descent(X, Y, epsilon, max_iter, 0.75)
model2 <- sgd(X, Y, epsilon , max_iter, 0.01)
model3 <- iwls(X, Y, epsilon, max_iter)
model4 <- optim_logreg(X, Y, method = "CG", epsilon, max_iter)
model5 <- optim_logreg(X, Y, method = "BFGS", epsilon, max_iter)
labels <- c("GD", "SGD", "IWLS", "CG", "BFGS")


df <- data.frame(algorithm=labels, jain=c(model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1]))

df2 <- data.frame(dataset = "Jain", model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1])

g1 <- compare_models(list(model,model2, model3, model4, model5), labels = labels , title = "Dataset Jain")
g1 <- g1 + theme(legend.position="bottom")

```

```{r jain_time}
micro1 <- microbenchmark(gd = gradient_descent(X, Y, epsilon, max_iter, 0.75),
                        sgd = sgd(X, Y, epsilon , max_iter, 0.01),
                        iwls = iwls(X, Y, epsilon, max_iter),
                        cg = optim_logreg(X, Y, method = "CG", epsilon, max_iter),
                        bfgs = optim_logreg(X, Y, method = "BFGS", epsilon, max_iter),
               times = 10, unit='ms')
time_stats1 <- cbind(summary(micro1)[, c(1,4)], iters=sapply(list(model, model2, model3, model4, model5), function(m) m$iters))
time_stats1 <- time_stats1 %>% mutate(ms_per_iter=mean/iters) %>% rename(mean_ms = mean)
```


```{r prep_models_spam, warning=FALSE}
X <- as.matrix(spambase_train[,-ncol(spambase_train)])
Y <- as.matrix(spambase_train[,ncol(spambase_train)])


max_iter <- 100
epsilon <- 1e-4

model <- gradient_descent(X, Y, epsilon, max_iter, 0.75)
model2 <- sgd(X, Y, epsilon , max_iter, 0.01)
model3 <- iwls(X, Y, epsilon, max_iter)
model4 <- optim_logreg(X, Y, method = "CG", epsilon, max_iter)
model5 <- optim_logreg(X, Y, method = "BFGS", epsilon, max_iter)

df <- cbind(df, data.frame(spambase=c(model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1])))
df2 <- rbind(df2, data.frame(dataset = "Spambase", model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1]))

g2 <- compare_models(list(model,model2, model3, model4, model5), labels = c("GD", "SGD", "IWLS", "CG", "BFGS"), title = "Dataset spambase")
g2 <- g2 + theme(legend.position="bottom")
```

```{r spam_time}
micro2 <- microbenchmark(gd = gradient_descent(X, Y, epsilon, max_iter, 0.75),
                         sgd = sgd(X, Y, epsilon , max_iter, 0.01),
                         iwls = iwls(X, Y, epsilon, max_iter),
                         cg = optim_logreg(X, Y, method = "CG", epsilon, max_iter),
                         bfgs = optim_logreg(X, Y, method = "BFGS", epsilon, max_iter),
                         times = 10, unit='s')
time_stats2 <- cbind(summary(micro2)[, c(1,4)], iters=sapply(list(model, model2, model3, model4, model5), function(m) m$iters))
time_stats2 <- time_stats2 %>% mutate(ms_per_iter=1000*mean/iters) %>% rename(mean_s = mean)
```

```{r time_comp}
kable(cbind(time_stats1, time_stats2[,-1]), row.names = FALSE, digits = 2, caption = "Training dataset's dimensions") %>% kable_styling(full_width = FALSE, latex_options = "hold_position") %>% add_header_above(c(" ", "Jain" = 3, "Spambase" = 3))
```


```{r prep_models_mamm, warning=FALSE}
X <- as.matrix(mammography_train[,-ncol(mammography_train)])
Y <- as.matrix(mammography_train[,ncol(mammography_train)])

max_iter <- 100
epsilon <- 0
lr <- 0.01

model <- gradient_descent(X, Y, epsilon, max_iter, 0.75)
model2 <- sgd(X, Y, epsilon , max_iter, lr)
model3 <- iwls(X, Y, epsilon, max_iter)
model4 <- optim_logreg(X, Y, method = "CG", epsilon, max_iter)
model5 <- optim_logreg(X, Y, method = "BFGS", epsilon, max_iter)

df <- cbind(df, data.frame(mammography=c(model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1])))
df2 <- rbind(df2, data.frame(dataset = "Mammography", model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1]))

g3 <- compare_models(list(model,model2, model3, model4, model5), labels = c("GD", "SGD", "IWLS", "CG", "BFGS"), title = "Dataset mammography")
g3 <- g3 + theme(legend.position="bottom")
```

```{r conv, fig.height=4, fig.width=9,  fig.cap="Values of loss during training for different algorithms"}
ggarrange(g1 , g2, g3, nrow=1)
```

```{r prep_models_skin, warning=FALSE, cache=TRUE}
X <- as.matrix(skin_seg_train[,-ncol(skin_seg_train)])
Y <- as.matrix(skin_seg_train[,ncol(skin_seg_train)])

max_iter <- 100
epsilon <- 0
lr <- 0.01

model <- gradient_descent(X, Y, epsilon, max_iter, 0.75)
model2 <- sgd(X, Y, epsilon , max_iter, lr)
model3 <- iwls(X, Y, epsilon, max_iter)
model4 <- optim_logreg(X, Y, method = "CG", epsilon, max_iter)
model5 <- optim_logreg(X, Y, method = "BFGS", epsilon, max_iter)


```

```{r}
df <- cbind(df, data.frame(skin_seg=c(model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1])))

df2 <- rbind(df2, data.frame(dataset = "Skin segmentation", model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1]))
```


```{r prep_models_occ, warning=FALSE}
X <- as.matrix(occupancy_train[,-ncol(occupancy_train)])
Y <- as.matrix(occupancy_train[,ncol(occupancy_train)])

max_iter <- 100
epsilon <- 0
lr <- 0.01

model <- gradient_descent(X, Y, epsilon, max_iter, 0.75)
model2 <- sgd(X, Y, epsilon , max_iter, lr)
model3 <- iwls(X, Y, epsilon, max_iter)
model4 <- optim_logreg(X, Y, method = "CG", epsilon, max_iter)
model5 <- optim_logreg(X, Y, method = "BFGS", epsilon, max_iter)

df <- cbind(df, data.frame(occupancy=c(model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1])))
df2 <- rbind(df2, data.frame(dataset = "Occupancy", model$costs[max_iter+1],model2$costs[max_iter+1], model3$costs[max_iter+1], model4$costs[max_iter+1], model5$costs[max_iter+1]))
```


```{r table}
knitr::kable(df, digits = 3, row.names = FALSE, caption = "Final loss for different algorithms on various datasets", col.names = c("Algorithm", "Jain", "Spambase", "Mammography","Skin segmentation", "Occupancy")) %>% kable_styling(full_width = FALSE, latex_options = "hold_position") %>% column_spec(1, border_right = TRUE)
```

## Model performance analysis
The aim of this experiment was to compare the performance of logistic regression with other popular models.

```{r get_measures, echo=FALSE}
get_measures <- function(X, Y, X_test, Y_test, do_knn=TRUE){
  gd.model <- gradient_descent(X, Y, 1e-4, 1000, 0.03)
  gd.pred <- predict(gd.model, X_test)
  sgd.model <- sgd(X, Y, 1e-4 , 1000, 0.01, verbose=FALSE)
  sgd.pred <- predict(sgd.model, X_test)
  iwls.model <- iwls(X, Y, 1e-4 , 1000)
  iwls.pred <- predict(iwls.model, X_test)
  bfgs.model <- optim_logreg(X, Y, "BFGS", 1e-4, 1000)
  bfgs.pred <- predict(bfgs.model, X_test)
  cg.model <- optim_logreg(X, Y, "CG", 1e-4)
  cg.pred <- predict(cg.model, X_test)
  
  lda.model <- lda(X, Y)
  lda.pred <- predict(lda.model, X_test)$class
  lda.pred <- as.numeric(levels(lda.pred))[lda.pred]
  qda.model <- qda(x=X, grouping = Y)
  qda.pred <- predict(qda.model, as.data.frame(X_test))$class
  qda.pred <- as.numeric(levels(qda.pred))[qda.pred]
  
  if(do_knn){
    knn1.pred <- knn(train = X, test = X_test, cl = Y, k = 1)
    knn1.pred <- as.numeric(levels(knn1.pred))[knn1.pred]
    knn3.pred <- knn(train = X, test = X_test, cl = Y, k = 3)
    knn3.pred <- as.numeric(levels(knn3.pred))[knn3.pred]
    knn5.pred <- knn(train = X, test = X_test, cl = Y, k = 5)
    knn5.pred <- as.numeric(levels(knn5.pred))[knn5.pred]
  }else{
    knn1.pred <- rep(NA, length.out=length(iwls.pred))
    knn3.pred <- rep(NA, length.out=length(iwls.pred))
    knn5.pred <- rep(NA, length.out=length(iwls.pred))
  }
  
  
  predictions <- list(gd = gd.pred, sgd = sgd.pred, iwls = iwls.pred, 
                      bfgs = bfgs.pred, cg = cg.pred, lda = lda.pred, 
                      qda = qda.pred, knn1 = knn1.pred, knn3 = knn3.pred, 
                      knn5 = knn5.pred)
  
  
  measures <- data.frame(t(sapply(predictions, 
                                  function(x) unlist(measure(Y_test, x)))))
  if(!do_knn){
    measures['knn1',] <- rep(0, length.out = ncol(measures))
    measures['knn3',] <- rep(0, length.out = ncol(measures))
    measures['knn5',] <- rep(0, length.out = ncol(measures))
  }
  
  measures['model'] <- rownames(measures)
  measures['model type'] <- c(rep('logistic regression', 5), 
                              rep('discriminatory analysis', 2),
                              rep('k nearest neighbors', 3))
  measures_melt <- reshape2::melt(measures, id.vars = c('model', 'model type'))
  measures_melt[measures_melt$variable %in% c('accuracy', 
                'f1_score', 'r2_score'),]
}

```

```{r jain, echo=FALSE, fig.width=10, fig.height=2.5, cache=TRUE}
X <- as.matrix(jain_train[,-ncol(jain_train)])
Y <- as.matrix(jain_train[,ncol(jain_train)])

X_test <- as.matrix(jain_test[,-ncol(jain_test)])
Y_test <- as.matrix(jain_test[,ncol(jain_test)])

measures <- get_measures(X, Y, X_test, Y_test)

ggplot(measures, 
       aes(x = reorder(model, -value), y = value, fill = `model type`)) + 
  geom_bar(stat='identity') + scale_fill_brewer(palette=1, type='qual') + 
  facet_wrap(vars(variable), scales='free', ncol = 3) + 
  labs(title='Jain', x='Model') +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1))
```

```{r spambase, echo=FALSE, fig.width=10, fig.height=2.5, cache=TRUE}
X <- as.matrix(spambase_train[,-ncol(spambase_train)])
Y <- as.matrix(spambase_train[,ncol(spambase_train)])

X_test <- as.matrix(spambase_test[,-ncol(spambase_test)])
Y_test <- as.matrix(spambase_test[,ncol(spambase_test)])

measures <- get_measures(X, Y, X_test, Y_test)

ggplot(measures, 
       aes(x = reorder(model, -value), y = value, fill = `model type`)) + 
  geom_bar(stat='identity') + scale_fill_brewer(palette=1, type='qual') + 
  facet_wrap(vars(variable), scales='free', ncol = 3) + 
  labs(title='Spambase', x='Model') +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1))
```

```{r mammography, echo=FALSE, fig.width=10, fig.height=2.5, cache=TRUE}
X <- as.matrix(mammography_train[,-ncol(mammography_train)])
Y <- as.matrix(mammography_train[,ncol(mammography_train)])

X_test <- as.matrix(mammography_test[,-ncol(mammography_test)])
Y_test <- as.matrix(mammography_test[,ncol(mammography_test)])

measures <- get_measures(X, Y, X_test, Y_test, do_knn = FALSE)

ggplot(measures, 
       aes(x = reorder(model, -value), y = value, fill = `model type`)) + 
  geom_bar(stat='identity') + scale_fill_brewer(palette=1, type='qual') + 
  facet_wrap(vars(variable), scales='free', ncol = 3) + 
  labs(title='Mammography', x='Model') +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1))
```

```{r skinseg, echo=FALSE, fig.width=10, fig.height=2.5, cache=TRUE}
X <- as.matrix(skin_seg_train[,-ncol(skin_seg_train)])
Y <- as.matrix(skin_seg_train[,ncol(skin_seg_train)])

X_test <- as.matrix(skin_seg_test[,-ncol(skin_seg_test)])
Y_test <- as.matrix(skin_seg_test[,ncol(skin_seg_test)])


measures <- get_measures(X, Y, X_test, Y_test, do_knn = FALSE)

ggplot(measures, 
       aes(x = reorder(model, -value), y = value, fill = `model type`)) + 
  geom_bar(stat='identity') + scale_fill_brewer(palette=1, type='qual') + 
  facet_wrap(vars(variable), scales='free', ncol = 3) + 
  labs(title='Skin segmentation', x='Model') +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1))
```

```{r occupancy, echo=FALSE, fig.width=10, fig.height=2.5, cache=TRUE}
X <- as.matrix(occupancy_train[,-ncol(occupancy_train)])
Y <- as.matrix(occupancy_train[,ncol(occupancy_train)])

X_test <- as.matrix(occupancy_test[,-ncol(occupancy_test)])
Y_test <- as.matrix(occupancy_test[,ncol(occupancy_test)])

measures <- get_measures(X, Y, X_test, Y_test)

ggplot(measures, 
       aes(x = reorder(model, -value), y = value, fill = `model type`)) + 
  geom_bar(stat='identity') + scale_fill_brewer(palette=1, type='qual') + 
  facet_wrap(vars(variable), scales='free', ncol = 3) + 
  labs(title='Occupancy', x='Model') +
  guides(fill=FALSE) +
  theme(axis.text.x = element_text(angle = 35, vjust = 1, hjust=1))
```

